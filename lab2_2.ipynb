{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZhuxinJiang/Gen-ai-course/blob/main/lab2_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key upgrades (still easy for students to understand):\n",
        "\n",
        "* Clearer SYSTEM prompt + rules (better role anchoring)\n",
        "* Less random decoding (more stable replies)\n",
        "* Stop generation when it tries to start a new User: turn (prevents runaway fake dialogue)\n",
        "* Cleans role leakage (removes stray Assistant: / User: tags)\n",
        "* Pads correctly (removes that pad_token_id warning)\n",
        "* Keeps context bounded (avoids slowing down over long chats)\n",
        "\n"
      ],
      "metadata": {
        "id": "axGmJy0MU8oz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCugmViiLM_v"
      },
      "outputs": [],
      "source": [
        "# Install necessary libraries for transformers, acceleration, Hugging Face Hub, and 4-bit quantization.\n",
        "!pip -q install \"transformers>=4.45.0\" \"accelerate>=0.33.0\" \"huggingface_hub>=0.24.0\" bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQg6ILyHMW0Z"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "\n",
        "# Retrieve the Hugging Face token from Colab's user data secrets.\n",
        "# This ensures sensitive API keys are not hardcoded in the notebook.\n",
        "hf_token = userdata.get(\"HF_TOKEN\")\n",
        "\n",
        "# Check if the token was successfully retrieved. If not, raise an error.\n",
        "# The user needs to add 'HF_TOKEN' to Colab Secrets (the key icon on the left panel).\n",
        "if not hf_token:\n",
        "    raise ValueError(\"HF_TOKEN not found. Add it in Colab Secrets (ðŸ”‘) and rerun.\")\n",
        "\n",
        "# Log in to the Hugging Face Hub using the retrieved token.\n",
        "# This authenticates the session, allowing access to models and datasets.\n",
        "login(token=hf_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_nFjvf94MvKN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "# Define the model identifier for the Llama 3.2 3B Instruct model.\n",
        "# This ID is used to fetch the pre-trained model and its tokenizer from Hugging Face.\n",
        "model_id = \"meta-llama/Llama-3.2-3B\"\n",
        "\n",
        "# Configure BitsAndBytes for 4-bit quantization.\n",
        "# This helps reduce memory usage by loading the model in 4-bit precision.\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,  # Enable 4-bit quantization.\n",
        "    bnb_4bit_compute_dtype=torch.float16, # Set the compute data type to float16 for performance.\n",
        ")\n",
        "\n",
        "# Load the tokenizer for the specified model.\n",
        "# The tokenizer is responsible for converting text into numerical tokens that the model can understand.\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
        "\n",
        "# Load the pre-trained causal language model.\n",
        "# device_map=\"auto\" automatically distributes the model across available devices (e.g., GPU).\n",
        "# quantization_config applies the BitsAndBytes configuration for memory-efficient loading.\n",
        "# torch_dtype=torch.float16 ensures the model is loaded with half-precision floating-point numbers.\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=bnb_config,\n",
        "    torch_dtype=torch.float16,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EbFAeyCkN7Q2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, StoppingCriteria, StoppingCriteriaList\n",
        "\n",
        "# --- Chatbot loop (Base model) ---\n",
        "# Note: Base models are trained to continue text, not follow instructions perfectly.\n",
        "# We can still make the chat feel more natural by:\n",
        "# 1) Using a clearer conversation template (SYSTEM / User / Assistant)\n",
        "# 2) Reducing randomness slightly\n",
        "# 3) Stopping generation when the model tries to start the next \"User:\" turn\n",
        "# 4) Cleaning up any role tags that leak into the output\n",
        "\n",
        "# Ensure pad_token exists (avoids generation warnings in many decoder-only models)\n",
        "if tokenizer.pad_token_id is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Define the system prompt for the chatbot, setting its persona and rules.\n",
        "SYSTEM_PROMPT = (\n",
        "    \"SYSTEM: You are a helpful teaching assistant for a university GenAI applications course.\\n\"\n",
        "    \"SYSTEM RULES:\\n\"\n",
        "    \"- Be friendly and natural.\\n\"\n",
        "    \"- Keep answers concise (about 3â€“6 sentences) unless the user asks for more.\\n\"\n",
        "    \"- If you are unsure, say so and ask a clarifying question.\\n\"\n",
        "    \"- Do not invent sources or citations.\\n\"\n",
        ")\n",
        "\n",
        "# Define strings that, if generated, indicate the model is trying to start a new speaker turn.\n",
        "STOP_STRINGS = [\"\\nUser:\", \"\\nSYSTEM:\", \"\\nAssistant:\"]\n",
        "\n",
        "# Helper function to convert stop strings into their token IDs.\n",
        "def _ids(s: str):\n",
        "    return tokenizer(s, add_special_tokens=False)[\"input_ids\"]\n",
        "\n",
        "# Convert stop strings to a list of token ID lists, filtering out any empty ones.\n",
        "STOP_IDS_LIST = [ids for ids in (_ids(s) for s in STOP_STRINGS) if ids]\n",
        "\n",
        "# Custom StoppingCriteria class to stop generation when a defined sequence of tokens is encountered.\n",
        "class StopOnSubsequence(StoppingCriteria):\n",
        "    def __init__(self, stop_ids_list):\n",
        "        self.stop_ids_list = [torch.tensor(x) for x in stop_ids_list]\n",
        "\n",
        "    def __call__(self, input_ids, scores, **kwargs):\n",
        "        # Iterate through all defined stop ID sequences.\n",
        "        for stop_ids in self.stop_ids_list:\n",
        "            n = len(stop_ids)\n",
        "            # Check if the current input_ids sequence is long enough to contain the stop sequence.\n",
        "            if input_ids.shape[1] >= n:\n",
        "                # Compare the end of the input_ids with the stop_ids.\n",
        "                if torch.all(input_ids[0, -n:] == stop_ids.to(input_ids.device)):\n",
        "                    return True  # Stop generation if a match is found.\n",
        "        return False\n",
        "\n",
        "# Create a StoppingCriteriaList with our custom stopping criterion.\n",
        "stopping = StoppingCriteriaList([StopOnSubsequence(STOP_IDS_LIST)])\n",
        "\n",
        "# Function to handle a single turn of the chat.\n",
        "def base_chat_once(user_text: str, transcript: str):\n",
        "    # Keep context bounded to prevent the input from growing too large and slowing down the model.\n",
        "    MAX_CHARS = 6000\n",
        "    if len(transcript) > MAX_CHARS:\n",
        "        transcript = transcript[-MAX_CHARS:]\n",
        "\n",
        "    # Construct the prompt for the model, including the system prompt, previous conversation, and current user input.\n",
        "    prompt = transcript + f\"\\nUser: {user_text}\\nAssistant:\"\n",
        "    # Tokenize the prompt and move it to the model's device (e.g., GPU).\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    # Generate a response from the model without tracking gradients (for inference).\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=180,  # Maximum number of new tokens to generate.\n",
        "            do_sample=True,      # Enable sampling for more varied responses.\n",
        "            temperature=0.4,     # Controls randomness (lower means less random).\n",
        "            top_p=0.9,           # Filters tokens by cumulative probability.\n",
        "            repetition_penalty=1.08, # Penalizes repeated tokens.\n",
        "            pad_token_id=tokenizer.pad_token_id, # Specify pad token ID.\n",
        "            eos_token_id=tokenizer.eos_token_id, # Specify end-of-sequence token ID.\n",
        "            stopping_criteria=stopping,  # Apply custom stopping criteria.\n",
        "        )\n",
        "\n",
        "    # Decode only the newly generated tokens to get the assistant's response.\n",
        "    new_tokens = output_ids[0][inputs[\"input_ids\"].shape[-1]:]\n",
        "    assistant_text = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
        "\n",
        "    # Clean up common \"role leakage\" where the model might generate role tags.\n",
        "    assistant_text = assistant_text.split(\"\\nUser:\")[0]\n",
        "    assistant_text = assistant_text.split(\"\\nSYSTEM:\")[0]\n",
        "    assistant_text = assistant_text.replace(\"Assistant:\", \"\").strip()\n",
        "\n",
        "    # Update the transcript with the latest turn for the next iteration.\n",
        "    transcript = prompt + \" \" + assistant_text\n",
        "    return assistant_text, transcript\n",
        "\n",
        "\n",
        "# Initialize conversation transcript with the system prompt.\n",
        "transcript = SYSTEM_PROMPT\n",
        "print(\"IN6241 Lab 2 (HF Base) â€” type 'exit' to stop.\\n\")\n",
        "\n",
        "# Main chat loop.\n",
        "while True:\n",
        "    user_text = input(\"You: \")  # Get user input.\n",
        "    if user_text.lower().strip() in (\"exit\", \"quit\"): # Check for exit commands.\n",
        "        print(\"Chat ended.\")\n",
        "        break\n",
        "\n",
        "    # Get assistant's response and updated transcript.\n",
        "    assistant_text, transcript = base_chat_once(user_text, transcript)\n",
        "    print(\"\\nAssistant:\", assistant_text, \"\\n\") # Print assistant's response."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}